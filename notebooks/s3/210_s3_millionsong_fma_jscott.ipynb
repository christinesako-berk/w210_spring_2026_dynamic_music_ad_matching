{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw1bAwxZ93Fb",
        "outputId": "bb234ae8-6b62-427e-eb9a-19248eb24041",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.42.40-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting botocore<1.43.0,>=1.42.40 (from boto3)\n",
            "  Downloading botocore-1.42.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.1.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3)\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.40->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.40->boto3) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.40->boto3) (1.17.0)\n",
            "Downloading boto3-1.42.40-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.40-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.42.40 botocore-1.42.40 jmespath-1.1.0 s3transfer-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to S3 Bucket"
      ],
      "metadata": {
        "id": "ck1QiKsDiFgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from google.colab import userdata\n",
        "\n",
        "# Pull credentials from Colab Secrets (no hardcoding)\n",
        "aws_access_key_id = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "aws_secret_access_key = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,\n",
        "    region_name='us-west-2'\n",
        ")\n",
        "\n",
        "# Quick sanity check — list objects in your bucket\n",
        "response = s3.list_objects_v2(Bucket='mids-capstone-music-ad-matching-2026', Prefix='raw-data/')\n",
        "for obj in response.get('Contents', []):\n",
        "    print(obj['Key'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xueh9SzXHm6k",
        "outputId": "f5859892-e703-4135-8893-06cfb16a904e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw-data/\n",
            "raw-data/fma/\n",
            "raw-data/meta-ads/\n",
            "raw-data/million-song/\n",
            "raw-data/synthetic-annotations/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: List buckets\n",
        "response = s3.list_buckets()\n",
        "print(\"Buckets:\", [b['Name'] for b in response['Buckets']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1-hiz7F9wfd",
        "outputId": "48b0bb0d-2006-4015-ae39-f27d4c8fad59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buckets: ['mids-capstone-music-ad-matching-2026']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Million Song Subset to S3"
      ],
      "metadata": {
        "id": "_OHclC1EjX2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "import boto3, os, tarfile\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# S3 client\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    region_name='us-west-2'\n",
        ")\n",
        "\n",
        "BUCKET = 'mids-capstone-music-ad-matching-2026'\n",
        "\n",
        "# Update this path to wherever you dropped it in Drive\n",
        "TARZ_PATH = '/content/drive/MyDrive/Colab_Notebooks/MIDS/210/millionsongsubset.tar.gz'\n",
        "\n",
        "# Extract\n",
        "print(\"Extracting...\")\n",
        "with tarfile.open(TARZ_PATH, 'r:gz') as tar:\n",
        "    tar.extractall('/content/msd_subset')\n",
        "\n",
        "# Upload HDF5 files\n",
        "print(\"Uploading HDF5 files to S3...\")\n",
        "count = 0\n",
        "for root, dirs, files in os.walk('/content/msd_subset'):\n",
        "    for f in files:\n",
        "        if f.endswith('.h5'):\n",
        "            local_path = os.path.join(root, f)\n",
        "            # Preserve the A/B/C/trackid.h5 structure\n",
        "            rel_path = local_path.split('msd_subset/')[-1]\n",
        "            s3_key = f'raw-data/million-song/{rel_path}'\n",
        "            s3.upload_file(local_path, BUCKET, s3_key)\n",
        "            count += 1\n",
        "            if count % 500 == 0:\n",
        "                print(f\"  ...{count} files uploaded\")\n",
        "\n",
        "print(f\"  ✓ Done: {count} HDF5 files\")\n",
        "\n",
        "# Upload SQLite additional files\n",
        "print(\"Uploading SQLite files...\")\n",
        "for root, dirs, files in os.walk('/content/msd_subset'):\n",
        "    for f in files:\n",
        "        if f.endswith('.db'):\n",
        "            local_path = os.path.join(root, f)\n",
        "            s3_key = f'raw-data/million-song/{f}'\n",
        "            s3.upload_file(local_path, BUCKET, s3_key)\n",
        "            print(f\"  ✓ {s3_key}\")\n",
        "\n",
        "# Verify\n",
        "print(\"\\n--- Verification ---\")\n",
        "response = s3.list_objects_v2(Bucket=BUCKET, Prefix='raw-data/million-song/')\n",
        "objects = response.get('Contents', [])\n",
        "total_mb = sum(o['Size'] for o in objects) / 1e6\n",
        "print(f\"raw-data/million-song/: {len(objects)} files, {total_mb:.1f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOKH4ZjmIV8-",
        "outputId": "273b9a77-d50c-4835-9fb1-8de62744648b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1892497262.py:23: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall('/content/msd_subset')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading HDF5 files to S3...\n",
            "  ...500 files uploaded\n",
            "  ...1000 files uploaded\n",
            "  ...1500 files uploaded\n",
            "  ...2000 files uploaded\n",
            "  ...2500 files uploaded\n",
            "  ...3000 files uploaded\n",
            "  ...3500 files uploaded\n",
            "  ...4000 files uploaded\n",
            "  ...4500 files uploaded\n",
            "  ...5000 files uploaded\n",
            "  ...5500 files uploaded\n",
            "  ...6000 files uploaded\n",
            "  ...6500 files uploaded\n",
            "  ...7000 files uploaded\n",
            "  ...7500 files uploaded\n",
            "  ...8000 files uploaded\n",
            "  ...8500 files uploaded\n",
            "  ...9000 files uploaded\n",
            "  ...9500 files uploaded\n",
            "  ...10000 files uploaded\n",
            "  ✓ Done: 10000 HDF5 files\n",
            "Uploading SQLite files...\n",
            "\n",
            "--- Verification ---\n",
            "raw-data/million-song/: 1000 files, 268.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load FMA Full Dataset to S3"
      ],
      "metadata": {
        "id": "fmEZHAYPkKX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os, zipfile, hashlib\n",
        "import urllib.request\n",
        "from google.colab import userdata\n",
        "\n",
        "# S3 client\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    region_name='us-west-2'\n",
        ")\n",
        "\n",
        "BUCKET = 'mids-capstone-music-ad-matching-2026'\n",
        "FMA_URL = 'https://os.unil.cloud.switch.ch/fma/fma_metadata.zip'\n",
        "EXPECTED_SHA1 = 'f0df49ffe5f2a6008d7dc83c6915b31835dfe733'\n",
        "\n",
        "# Download\n",
        "print(\"Downloading fma_metadata.zip (342 MB)...\")\n",
        "urllib.request.urlretrieve(FMA_URL, 'fma_metadata.zip')\n",
        "print(f\"  Downloaded: {os.path.getsize('fma_metadata.zip') / 1e6:.1f} MB\")\n",
        "\n",
        "# Verify integrity\n",
        "print(\"Verifying sha1...\")\n",
        "sha1 = hashlib.sha1()\n",
        "with open('fma_metadata.zip', 'rb') as f:\n",
        "    while chunk := f.read(8192):\n",
        "        sha1.update(chunk)\n",
        "actual_sha1 = sha1.hexdigest()\n",
        "assert actual_sha1 == EXPECTED_SHA1, f\"SHA1 mismatch! Expected {EXPECTED_SHA1}, got {actual_sha1}\"\n",
        "print(f\"  ✓ SHA1 verified: {actual_sha1}\")\n",
        "\n",
        "# Extract\n",
        "print(\"Extracting...\")\n",
        "with zipfile.ZipFile('fma_metadata.zip', 'r') as z:\n",
        "    z.extractall('/content/fma_metadata')\n",
        "    print(f\"  Contents: {z.namelist()}\")\n",
        "\n",
        "# Upload CSVs to S3\n",
        "FMA_FILES = ['tracks.csv', 'genres.csv', 'features.csv', 'echonest.csv']\n",
        "print(\"Uploading to S3...\")\n",
        "for f in FMA_FILES:\n",
        "    local_path = f'/content/fma_metadata/fma_metadata/{f}'  # nested folder\n",
        "    s3_key = f'raw-data/fma/{f}'\n",
        "    if os.path.exists(local_path):\n",
        "        s3.upload_file(local_path, BUCKET, s3_key)\n",
        "        print(f\"  ✓ {s3_key} ({os.path.getsize(local_path) / 1e6:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"  ✗ {f} not found\")\n",
        "\n",
        "# Verify S3\n",
        "print(\"\\n--- S3 Verification ---\")\n",
        "response = s3.list_objects_v2(Bucket=BUCKET, Prefix='raw-data/fma/')\n",
        "for obj in response.get('Contents', []):\n",
        "    print(f\"  {obj['Key']} ({obj['Size'] / 1e6:.1f} MB)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1OBMD2GIV6K",
        "outputId": "e4214671-6063-42ab-8f66-498e2a02c741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fma_metadata.zip (342 MB)...\n",
            "  Downloaded: 358.4 MB\n",
            "Verifying sha1...\n",
            "  ✓ SHA1 verified: f0df49ffe5f2a6008d7dc83c6915b31835dfe733\n",
            "Extracting...\n",
            "  Contents: ['fma_metadata/README.txt', 'fma_metadata/checksums', 'fma_metadata/not_found.pickle', 'fma_metadata/raw_genres.csv', 'fma_metadata/raw_albums.csv', 'fma_metadata/raw_artists.csv', 'fma_metadata/raw_tracks.csv', 'fma_metadata/tracks.csv', 'fma_metadata/genres.csv', 'fma_metadata/raw_echonest.csv', 'fma_metadata/echonest.csv', 'fma_metadata/features.csv']\n",
            "Uploading to S3...\n",
            "  ✓ raw-data/fma/tracks.csv (260.4 MB)\n",
            "  ✓ raw-data/fma/genres.csv (0.0 MB)\n",
            "  ✓ raw-data/fma/features.csv (951.1 MB)\n",
            "  ✓ raw-data/fma/echonest.csv (44.0 MB)\n",
            "\n",
            "--- S3 Verification ---\n",
            "  raw-data/fma/ (0.0 MB)\n",
            "  raw-data/fma/echonest.csv (44.0 MB)\n",
            "  raw-data/fma/features.csv (951.1 MB)\n",
            "  raw-data/fma/genres.csv (0.0 MB)\n",
            "  raw-data/fma/tracks.csv (260.4 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TypZ3l-pvYMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load IAB Content Taxonomy to S3"
      ],
      "metadata": {
        "id": "D6PYdhYanmM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "import urllib.request\n",
        "from google.colab import userdata\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    region_name='us-west-2'\n",
        ")\n",
        "\n",
        "BUCKET = 'mids-capstone-music-ad-matching-2026'\n",
        "\n",
        "# Raw GitHub URL for the TSV\n",
        "IAB_URL = 'https://raw.githubusercontent.com/InteractiveAdvertisingBureau/Taxonomies/develop/Content%20Taxonomies/Content%20Taxonomy%203.1.tsv'\n",
        "\n",
        "print(\"Downloading IAB Content Taxonomy 3.1...\")\n",
        "urllib.request.urlretrieve(IAB_URL, 'Content_Taxonomy_3.1.tsv')\n",
        "size = os.path.getsize('Content_Taxonomy_3.1.tsv')\n",
        "print(f\"  Downloaded: {size / 1e3:.1f} KB\")\n",
        "\n",
        "# Upload to meta-ads folder since this is the ad taxonomy\n",
        "s3_key = 'raw-data/meta-ads/IAB_Content_Taxonomy_3.1.tsv'\n",
        "s3.upload_file('Content_Taxonomy_3.1.tsv', BUCKET, s3_key)\n",
        "print(f\"  ✓ Uploaded: {s3_key}\")\n",
        "\n",
        "# Quick peek at the structure\n",
        "import pandas as pd\n",
        "df = pd.read_csv('Content_Taxonomy_3.1.tsv', sep='\\t')\n",
        "print(f\"\\n  Shape: {df.shape}\")\n",
        "print(f\"  Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\n{df.head(10)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y16-HU-PIV3m",
        "outputId": "8af55fe6-d15a-4905-ae7d-eb2da247d75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading IAB Content Taxonomy 3.1...\n",
            "  Downloaded: 47.3 KB\n",
            "  ✓ Uploaded: raw-data/meta-ads/IAB_Content_Taxonomy_3.1.tsv\n",
            "\n",
            "  Shape: (705, 8)\n",
            "  Columns: ['Relational ID System', 'Unnamed: 1', 'Unnamed: 2', 'Content Taxonomy v3.1 Tiered Categories', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Extension']\n",
            "\n",
            "  Relational ID System Unnamed: 1                        Unnamed: 2  \\\n",
            "0            Unique ID     Parent                              Name   \n",
            "1                  150        NaN                       Attractions   \n",
            "2                  151        150         Amusement and Theme Parks   \n",
            "3                  179        150                Bars & Restaurants   \n",
            "4                  181        150                Casinos & Gambling   \n",
            "5                  153        150  Historic Site and Landmark Tours   \n",
            "6                  154        150          Malls & Shopping Centers   \n",
            "7                  155        150               Museums & Galleries   \n",
            "8                  158        150                        Nightclubs   \n",
            "9                  159        150                Outdoor Activities   \n",
            "\n",
            "  Content Taxonomy v3.1 Tiered Categories                        Unnamed: 4  \\\n",
            "0                                  Tier 1                            Tier 2   \n",
            "1                             Attractions                               NaN   \n",
            "2                             Attractions         Amusement and Theme Parks   \n",
            "3                             Attractions                Bars & Restaurants   \n",
            "4                             Attractions                Casinos & Gambling   \n",
            "5                             Attractions  Historic Site and Landmark Tours   \n",
            "6                             Attractions          Malls & Shopping Centers   \n",
            "7                             Attractions               Museums & Galleries   \n",
            "8                             Attractions                        Nightclubs   \n",
            "9                             Attractions                Outdoor Activities   \n",
            "\n",
            "  Unnamed: 5 Unnamed: 6 Extension  \n",
            "0     Tier 3     Tier 4       NaN  \n",
            "1        NaN        NaN       NaN  \n",
            "2        NaN        NaN       NaN  \n",
            "3        NaN        NaN       NaN  \n",
            "4        NaN        NaN       NaN  \n",
            "5        NaN        NaN       NaN  \n",
            "6        NaN        NaN       NaN  \n",
            "7        NaN        NaN       NaN  \n",
            "8        NaN        NaN       NaN  \n",
            "9        NaN        NaN       NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuPSMiR9IVwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: Upload test csv to your bucket\n",
        "s3.put_object(\n",
        "    Bucket='mids-capstone-music-ad-matching-2026',\n",
        "    Key='test-uploads/test-from-jason.txt',\n",
        "    Body='Hello from Jason - IAM user test'\n",
        ")\n",
        "print(\"Upload successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6nD5UH9wrg",
        "outputId": "c7c03c04-09b6-492e-ccd7-0ce72afedbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consolidate Million Song HDF5 Files to Metadata CSV"
      ],
      "metadata": {
        "id": "4QBil0NTivGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py -q\n",
        "\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import boto3, os\n",
        "from google.colab import userdata\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    region_name='us-west-2'\n",
        ")\n",
        "\n",
        "BUCKET = 'mids-capstone-music-ad-matching-2026'\n",
        "\n",
        "# First, let's look at one HDF5 file to see what fields are available\n",
        "# Download a single file to inspect\n",
        "sample_key = None\n",
        "response = s3.list_objects_v2(Bucket=BUCKET, Prefix='raw-data/million-song/', MaxKeys=10)\n",
        "for obj in response.get('Contents', []):\n",
        "    if obj['Key'].endswith('.h5'):\n",
        "        sample_key = obj['Key']\n",
        "        print(f\"Found: {sample_key}\")\n",
        "        break\n",
        "\n",
        "s3.download_file(BUCKET, sample_key, 'sample.h5')\n",
        "\n",
        "with h5py.File('sample.h5', 'r') as f:\n",
        "    def print_structure(name, obj):\n",
        "        print(name)\n",
        "    f.visititems(print_structure)"
      ],
      "metadata": {
        "id": "IVEMli3g3Kk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601d2105-c8a5-43a1-cf92-3695b8968b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found: raw-data/million-song/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5\n",
            "analysis\n",
            "analysis/bars_confidence\n",
            "analysis/bars_start\n",
            "analysis/beats_confidence\n",
            "analysis/beats_start\n",
            "analysis/sections_confidence\n",
            "analysis/sections_start\n",
            "analysis/segments_confidence\n",
            "analysis/segments_loudness_max\n",
            "analysis/segments_loudness_max_time\n",
            "analysis/segments_loudness_start\n",
            "analysis/segments_pitches\n",
            "analysis/segments_start\n",
            "analysis/segments_timbre\n",
            "analysis/songs\n",
            "analysis/tatums_confidence\n",
            "analysis/tatums_start\n",
            "metadata\n",
            "metadata/artist_terms\n",
            "metadata/artist_terms_freq\n",
            "metadata/artist_terms_weight\n",
            "metadata/similar_artists\n",
            "metadata/songs\n",
            "musicbrainz\n",
            "musicbrainz/artist_mbtags\n",
            "musicbrainz/artist_mbtags_count\n",
            "musicbrainz/songs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File('sample.h5', 'r') as f:\n",
        "    print(\"=== metadata/songs ===\")\n",
        "    print(f['metadata/songs'].dtype.names)\n",
        "    print(f['metadata/songs'][0])\n",
        "\n",
        "    print(\"\\n=== analysis/songs ===\")\n",
        "    print(f['analysis/songs'].dtype.names)\n",
        "    print(f['analysis/songs'][0])\n",
        "\n",
        "    print(\"\\n=== artist_terms ===\")\n",
        "    print(f['metadata/artist_terms'][:])\n",
        "\n",
        "    print(\"\\n=== artist_terms_freq ===\")\n",
        "    print(f['metadata/artist_terms_freq'][:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHa6lgENittB",
        "outputId": "200357ee-031b-4ffc-9777-511c420d5554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== metadata/songs ===\n",
            "('analyzer_version', 'artist_7digitalid', 'artist_familiarity', 'artist_hotttnesss', 'artist_id', 'artist_latitude', 'artist_location', 'artist_longitude', 'artist_mbid', 'artist_name', 'artist_playmeid', 'genre', 'idx_artist_terms', 'idx_similar_artists', 'release', 'release_7digitalid', 'song_hotttnesss', 'song_id', 'title', 'track_7digitalid')\n",
            "(b'', 165270, 0.5817937658450281, 0.4019975433642836, b'ARD7TVE1187B99BFB1', nan, b'California - LA', nan, b'e77e51a5-4761-45b3-9847-2051f811e366', b'Casual', 4479, b'', 0, 0, b'Fear Itself', 300848, 0.6021199899057548, b'SOMZWCG12A8C13C480', b\"I Didn't Mean To\", 3401791)\n",
            "\n",
            "=== analysis/songs ===\n",
            "('analysis_sample_rate', 'audio_md5', 'danceability', 'duration', 'end_of_fade_in', 'energy', 'idx_bars_confidence', 'idx_bars_start', 'idx_beats_confidence', 'idx_beats_start', 'idx_sections_confidence', 'idx_sections_start', 'idx_segments_confidence', 'idx_segments_loudness_max', 'idx_segments_loudness_max_time', 'idx_segments_loudness_start', 'idx_segments_pitches', 'idx_segments_start', 'idx_segments_timbre', 'idx_tatums_confidence', 'idx_tatums_start', 'key', 'key_confidence', 'loudness', 'mode', 'mode_confidence', 'start_of_fade_out', 'tempo', 'time_signature', 'time_signature_confidence', 'track_id')\n",
            "(22050, b'a222795e07cd65b7a530f1346f520649', 0.0, 218.93179, 0.247, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.736, -11.197, 0, 0.636, 218.932, 92.198, 4, 0.778, b'TRAAAAW128F429D538')\n",
            "\n",
            "=== artist_terms ===\n",
            "[b'hip hop' b'underground rap' b'g funk' b'alternative rap' b'gothic rock'\n",
            " b'west coast rap' b'rap' b'club dance' b'singer-songwriter' b'chill-out'\n",
            " b'underground hip hop' b'rock' b'gothic' b'san francisco bay area'\n",
            " b'indie' b'american' b'punk' b'california' b'industrial' b'new york'\n",
            " b'90s' b'latin' b'spanish' b'dark' b'ebm' b'underground' b'deathrock'\n",
            " b'west coast' b'san francisco' b'producer' b'oakland' b'catalan'\n",
            " b'barcelona' b'doomsdope' b'norcal' b'west coast hip hop'\n",
            " b'alternative rock']\n",
            "\n",
            "=== artist_terms_freq ===\n",
            "[1.         0.77613623 0.72966979 0.68301072 0.73013328 0.6715377\n",
            " 0.80834839 0.63365545 0.72966979 0.5663102  0.57592584 0.76900909\n",
            " 0.58208775 0.47493557 0.67402276 0.61640511 0.64964936 0.51614094\n",
            " 0.6000771  0.51980619 0.59353997 0.56511801 0.55205485 0.53260669\n",
            " 0.53958053 0.50931501 0.45188402 0.4484606  0.43320144 0.4322592\n",
            " 0.42699224 0.40636693 0.36872193 0.35580011 0.33148334 0.28803951\n",
            " 0.3212017 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import boto3, os\n",
        "from google.colab import userdata\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    region_name='us-west-2'\n",
        ")\n",
        "\n",
        "BUCKET = 'mids-capstone-music-ad-matching-2026'\n",
        "\n",
        "def extract_song(filepath):\n",
        "    \"\"\"Extract relevant fields from a single MSD HDF5 file.\"\"\"\n",
        "    try:\n",
        "        with h5py.File(filepath, 'r') as f:\n",
        "            meta = f['metadata/songs'][0]\n",
        "            analysis = f['analysis/songs'][0]\n",
        "\n",
        "            # Artist terms (tags) — join as pipe-delimited string\n",
        "            terms = f['metadata/artist_terms'][:]\n",
        "            terms_freq = f['metadata/artist_terms_freq'][:]\n",
        "\n",
        "            # Decode byte strings\n",
        "            terms_str = '|'.join([t.decode('utf-8', errors='ignore') for t in terms])\n",
        "\n",
        "            return {\n",
        "                # Metadata\n",
        "                'track_id': analysis['track_id'].decode(),\n",
        "                'song_id': meta['song_id'].decode(),\n",
        "                'artist_id': meta['artist_id'].decode(),\n",
        "                'artist_name': meta['artist_name'].decode('utf-8', errors='ignore'),\n",
        "                'title': meta['title'].decode('utf-8', errors='ignore'),\n",
        "                'release': meta['release'].decode('utf-8', errors='ignore'),\n",
        "                'artist_location': meta['artist_location'].decode('utf-8', errors='ignore'),\n",
        "                'artist_latitude': meta['artist_latitude'],\n",
        "                'artist_longitude': meta['artist_longitude'],\n",
        "                'artist_familiarity': meta['artist_familiarity'],\n",
        "                'artist_hotttnesss': meta['artist_hotttnesss'],\n",
        "                'song_hotttnesss': meta['song_hotttnesss'],\n",
        "                'genre': meta['genre'].decode('utf-8', errors='ignore'),\n",
        "\n",
        "                # Audio analysis\n",
        "                'duration': analysis['duration'],\n",
        "                'tempo': analysis['tempo'],\n",
        "                'loudness': analysis['loudness'],\n",
        "                'danceability': analysis['danceability'],\n",
        "                'energy': analysis['energy'],\n",
        "                'key': analysis['key'],\n",
        "                'key_confidence': analysis['key_confidence'],\n",
        "                'mode': analysis['mode'],\n",
        "                'mode_confidence': analysis['mode_confidence'],\n",
        "                'time_signature': analysis['time_signature'],\n",
        "                'time_signature_confidence': analysis['time_signature_confidence'],\n",
        "                'start_of_fade_out': analysis['start_of_fade_out'],\n",
        "                'end_of_fade_in': analysis['end_of_fade_in'],\n",
        "\n",
        "                # Tags\n",
        "                'artist_terms': terms_str,\n",
        "                'num_artist_terms': len(terms),\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"  Error reading {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Download all HDF5 files and extract\n",
        "print(\"Listing all HDF5 files in S3...\")\n",
        "paginator = s3.get_paginator('list_objects_v2')\n",
        "h5_keys = []\n",
        "for page in paginator.paginate(Bucket=BUCKET, Prefix='raw-data/million-song/'):\n",
        "    for obj in page.get('Contents', []):\n",
        "        if obj['Key'].endswith('.h5'):\n",
        "            h5_keys.append(obj['Key'])\n",
        "print(f\"  Found {len(h5_keys)} HDF5 files\")\n",
        "\n",
        "# Download and extract in batches\n",
        "rows = []\n",
        "batch_size = 500\n",
        "for i, key in enumerate(h5_keys):\n",
        "    local_path = f'/content/msd_temp/{os.path.basename(key)}'\n",
        "    os.makedirs('/content/msd_temp', exist_ok=True)\n",
        "    s3.download_file(BUCKET, key, local_path)\n",
        "\n",
        "    row = extract_song(local_path)\n",
        "    if row:\n",
        "        rows.append(row)\n",
        "\n",
        "    # Clean up to save disk space\n",
        "    os.remove(local_path)\n",
        "\n",
        "    if (i + 1) % batch_size == 0:\n",
        "        print(f\"  ...processed {i + 1}/{len(h5_keys)} files\")\n",
        "\n",
        "# Build DataFrame\n",
        "print(f\"\\nBuilding DataFrame from {len(rows)} songs...\")\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.shape)\n",
        "print(df.head())\n",
        "print(df.dtypes)\n",
        "\n",
        "# Save locally and upload to processed-data/\n",
        "output_path = '/content/million_song_subset.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"\\nSaved: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n",
        "\n",
        "s3_key = 'processed-data/million_song_subset.csv'\n",
        "s3.upload_file(output_path, BUCKET, s3_key)\n",
        "print(f\"  ✓ Uploaded: {s3_key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aWpnXTLjmJE",
        "outputId": "5cb95fad-46bb-41c0-d374-641ec672383d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing all HDF5 files in S3...\n",
            "  Found 10000 HDF5 files\n",
            "  ...processed 500/10000 files\n",
            "  ...processed 1000/10000 files\n",
            "  ...processed 1500/10000 files\n",
            "  ...processed 2000/10000 files\n",
            "  ...processed 2500/10000 files\n",
            "  ...processed 3000/10000 files\n",
            "  ...processed 3500/10000 files\n",
            "  ...processed 4000/10000 files\n",
            "  ...processed 4500/10000 files\n",
            "  ...processed 5000/10000 files\n",
            "  ...processed 5500/10000 files\n",
            "  ...processed 6000/10000 files\n",
            "  ...processed 6500/10000 files\n",
            "  ...processed 7000/10000 files\n",
            "  ...processed 7500/10000 files\n",
            "  ...processed 8000/10000 files\n",
            "  ...processed 8500/10000 files\n",
            "  ...processed 9000/10000 files\n",
            "  ...processed 9500/10000 files\n",
            "  ...processed 10000/10000 files\n",
            "\n",
            "Building DataFrame from 10000 songs...\n",
            "(10000, 28)\n",
            "             track_id             song_id           artist_id  \\\n",
            "0  TRAAAAW128F429D538  SOMZWCG12A8C13C480  ARD7TVE1187B99BFB1   \n",
            "1  TRAAABD128F429CF47  SOCIWDW12A8C13D406  ARMJAGH1187FB546F3   \n",
            "2  TRAAADZ128F9348C2E  SOXVLOJ12AB0189215  ARKRRTF1187B9984DA   \n",
            "3  TRAAAEF128F4273421  SONHOTT12A8C13493C  AR7G5I41187FB4CE6C   \n",
            "4  TRAAAFD128F92F423A  SOFSOCN12A8C143F5D  ARXR32B1187FB57099   \n",
            "\n",
            "        artist_name             title                              release  \\\n",
            "0            Casual  I Didn't Mean To                          Fear Itself   \n",
            "1      The Box Tops         Soul Deep                           Dimensions   \n",
            "2  Sonora Santanera   Amor De Cabaret  Las Numero 1 De La Sonora Santanera   \n",
            "3          Adam Ant   Something Girls                        Friend Or Foe   \n",
            "4               Gob    Face the Ashes                        Muertos Vivos   \n",
            "\n",
            "   artist_location  artist_latitude  artist_longitude  artist_familiarity  \\\n",
            "0  California - LA              NaN               NaN            0.581794   \n",
            "1      Memphis, TN         35.14968         -90.04892            0.630630   \n",
            "2                               NaN               NaN            0.487357   \n",
            "3  London, England              NaN               NaN            0.630382   \n",
            "4                               NaN               NaN            0.651046   \n",
            "\n",
            "   ...  key  key_confidence mode  mode_confidence  time_signature  \\\n",
            "0  ...    1           0.736    0            0.636               4   \n",
            "1  ...    6           0.169    0            0.430               4   \n",
            "2  ...    8           0.643    1            0.565               1   \n",
            "3  ...    0           0.751    1            0.749               4   \n",
            "4  ...    2           0.092    1            0.371               4   \n",
            "\n",
            "   time_signature_confidence  start_of_fade_out  end_of_fade_in  \\\n",
            "0                      0.778            218.932           0.247   \n",
            "1                      0.384            137.915           0.148   \n",
            "2                      0.000            172.304           0.282   \n",
            "3                      0.000            217.124           0.000   \n",
            "4                      0.562            198.699           0.066   \n",
            "\n",
            "                                        artist_terms  num_artist_terms  \n",
            "0  hip hop|underground rap|g funk|alternative rap...                37  \n",
            "1  blue-eyed soul|pop rock|blues-rock|beach music...                38  \n",
            "2  salsa|cumbia|tejano|ranchera|latin pop|latin|t...                10  \n",
            "3  pop rock|new wave|dance rock|rock|new romantic...                43  \n",
            "4  pop punk|ska punk|breakcore|alternative metal|...                38  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "track_id                      object\n",
            "song_id                       object\n",
            "artist_id                     object\n",
            "artist_name                   object\n",
            "title                         object\n",
            "release                       object\n",
            "artist_location               object\n",
            "artist_latitude              float64\n",
            "artist_longitude             float64\n",
            "artist_familiarity           float64\n",
            "artist_hotttnesss            float64\n",
            "song_hotttnesss              float64\n",
            "genre                         object\n",
            "duration                     float64\n",
            "tempo                        float64\n",
            "loudness                     float64\n",
            "danceability                 float64\n",
            "energy                       float64\n",
            "key                            int32\n",
            "key_confidence               float64\n",
            "mode                           int32\n",
            "mode_confidence              float64\n",
            "time_signature                 int32\n",
            "time_signature_confidence    float64\n",
            "start_of_fade_out            float64\n",
            "end_of_fade_in               float64\n",
            "artist_terms                  object\n",
            "num_artist_terms               int64\n",
            "dtype: object\n",
            "\n",
            "Saved: 5.0 MB\n",
            "  ✓ Uploaded: processed-data/million_song_subset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Million Songs Segments Data to CSV"
      ],
      "metadata": {
        "id": "xVh1oaVVu-TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import boto3, os\n",
        "from google.colab import userdata\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    region_name='us-west-2'\n",
        ")\n",
        "\n",
        "BUCKET = 'mids-capstone-music-ad-matching-2026'\n",
        "\n",
        "def extract_segments(filepath):\n",
        "    \"\"\"Extract per-segment timbre, pitch, and loudness arrays.\"\"\"\n",
        "    try:\n",
        "        with h5py.File(filepath, 'r') as f:\n",
        "            song_id = f['metadata/songs'][0]['song_id'].decode()\n",
        "\n",
        "            segments_start = f['analysis/segments_start'][:]\n",
        "            segments_timbre = f['analysis/segments_timbre'][:]    # (n_segments, 12)\n",
        "            segments_pitches = f['analysis/segments_pitches'][:]  # (n_segments, 12)\n",
        "            segments_loudness_max = f['analysis/segments_loudness_max'][:]\n",
        "\n",
        "            rows = []\n",
        "            for i in range(len(segments_start)):\n",
        "                row = {\n",
        "                    'song_id': song_id,\n",
        "                    'segment_idx': i,\n",
        "                    'segment_start': segments_start[i],\n",
        "                    'loudness_max': segments_loudness_max[i],\n",
        "                }\n",
        "                # Timbre dimensions\n",
        "                for j in range(12):\n",
        "                    row[f'timbre_{j}'] = segments_timbre[i][j]\n",
        "                # Pitch chroma dimensions\n",
        "                for j in range(12):\n",
        "                    row[f'pitch_{j}'] = segments_pitches[i][j]\n",
        "                rows.append(row)\n",
        "\n",
        "            return rows\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {filepath}: {e}\")\n",
        "        return []\n",
        "\n",
        "# List all HDF5 keys\n",
        "print(\"Listing HDF5 files...\")\n",
        "paginator = s3.get_paginator('list_objects_v2')\n",
        "h5_keys = []\n",
        "for page in paginator.paginate(Bucket=BUCKET, Prefix='raw-data/million-song/'):\n",
        "    for obj in page.get('Contents', []):\n",
        "        if obj['Key'].endswith('.h5'):\n",
        "            h5_keys.append(obj['Key'])\n",
        "print(f\"  Found {len(h5_keys)} HDF5 files\")\n",
        "\n",
        "# Extract segments\n",
        "all_rows = []\n",
        "for i, key in enumerate(h5_keys):\n",
        "    local_path = f'/content/msd_temp/{os.path.basename(key)}'\n",
        "    os.makedirs('/content/msd_temp', exist_ok=True)\n",
        "    s3.download_file(BUCKET, key, local_path)\n",
        "\n",
        "    rows = extract_segments(local_path)\n",
        "    all_rows.extend(rows)\n",
        "    os.remove(local_path)\n",
        "\n",
        "    if (i + 1) % 500 == 0:\n",
        "        print(f\"  ...processed {i + 1}/{len(h5_keys)} files, {len(all_rows):,} segments so far\")\n",
        "\n",
        "# Build and save\n",
        "print(f\"\\nBuilding DataFrame from {len(all_rows):,} segments...\")\n",
        "df_segments = pd.DataFrame(all_rows)\n",
        "print(f\"  Shape: {df_segments.shape}\")\n",
        "print(df_segments.head())\n",
        "\n",
        "output_path = '/content/million_song_subset_segments.csv'\n",
        "df_segments.to_csv(output_path, index=False)\n",
        "print(f\"\\nSaved: {os.path.getsize(output_path) / 1e6:.1f} MB\")\n",
        "\n",
        "s3_key = 'processed-data/million_song_subset_segments.csv'\n",
        "s3.upload_file(output_path, BUCKET, s3_key)\n",
        "print(f\"  ✓ Uploaded: {s3_key}\")"
      ],
      "metadata": {
        "id": "iAm0_M5KjmEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95095d22-7944-456a-c84d-c6f9f1ad1a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing HDF5 files...\n",
            "  Found 10000 HDF5 files\n",
            "  ...processed 500/10000 files, 424,567 segments so far\n",
            "  ...processed 1000/10000 files, 846,275 segments so far\n",
            "  ...processed 1500/10000 files, 1,279,106 segments so far\n",
            "  ...processed 2000/10000 files, 1,715,918 segments so far\n",
            "  ...processed 2500/10000 files, 2,127,161 segments so far\n",
            "  ...processed 3000/10000 files, 2,558,562 segments so far\n",
            "  ...processed 3500/10000 files, 2,987,276 segments so far\n",
            "  ...processed 4000/10000 files, 3,428,612 segments so far\n",
            "  ...processed 4500/10000 files, 3,858,940 segments so far\n",
            "  ...processed 5000/10000 files, 4,291,283 segments so far\n",
            "  ...processed 5500/10000 files, 4,718,649 segments so far\n",
            "  ...processed 6000/10000 files, 5,150,237 segments so far\n",
            "  ...processed 6500/10000 files, 5,578,771 segments so far\n",
            "  ...processed 7000/10000 files, 6,013,690 segments so far\n",
            "  ...processed 7500/10000 files, 6,432,548 segments so far\n",
            "  ...processed 8000/10000 files, 6,846,573 segments so far\n",
            "  ...processed 8500/10000 files, 7,261,661 segments so far\n",
            "  ...processed 9000/10000 files, 7,690,555 segments so far\n",
            "  ...processed 9500/10000 files, 8,133,832 segments so far\n",
            "  ...processed 10000/10000 files, 8,577,406 segments so far\n",
            "\n",
            "Building DataFrame from 8,577,406 segments...\n",
            "  Shape: (8577406, 28)\n",
            "              song_id  segment_idx  segment_start  loudness_max  timbre_0  \\\n",
            "0  SOMZWCG12A8C13C480            0        0.00000       -60.000     0.000   \n",
            "1  SOMZWCG12A8C13C480            1        0.24671       -31.646    19.991   \n",
            "2  SOMZWCG12A8C13C480            2        0.47116       -34.565    20.597   \n",
            "3  SOMZWCG12A8C13C480            3        0.80376       -38.407    20.908   \n",
            "4  SOMZWCG12A8C13C480            4        0.89551       -34.696    22.173   \n",
            "\n",
            "   timbre_1  timbre_2  timbre_3  timbre_4  timbre_5  ...  pitch_2  pitch_3  \\\n",
            "0   171.130     9.469   -28.480    57.491   -50.067  ...    0.679    0.941   \n",
            "1  -143.504  -118.249  -142.909   -18.528    -4.209  ...    0.015    0.021   \n",
            "2  -203.829  -159.915   -89.765    29.646   -45.432  ...    1.000    0.549   \n",
            "3  -201.426  -151.280   -97.035    21.393   -58.436  ...    0.525    0.523   \n",
            "4  -213.799  -150.301   -58.292    16.520   -48.170  ...    0.389    0.569   \n",
            "\n",
            "   pitch_4  pitch_5  pitch_6  pitch_7  pitch_8  pitch_9  pitch_10  pitch_11  \n",
            "0    0.744    0.633    0.719    0.627    0.737    0.732     1.000     0.742  \n",
            "1    0.067    0.170    1.000    0.233    0.010    0.008     0.012     0.017  \n",
            "2    0.213    0.851    0.904    0.292    0.121    0.105     0.064     0.190  \n",
            "3    0.979    1.000    0.410    0.412    0.083    0.090     0.037     0.050  \n",
            "4    1.000    0.584    0.231    0.058    0.054    0.046     0.022     0.142  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "\n",
            "Saved: 1662.3 MB\n",
            "  ✓ Uploaded: processed-data/million_song_subset_segments.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4iAVltnjE_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}